{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Dataset Collection\n",
    "Please note that we have online [Kaggle notebook](https://www.kaggle.com/jacksoncrow/data-collection-demo) to avoid extra efforts of setting up the environment. In case you want to run this locally, you can easily set up the environment with our docker image. Please refer for the instructions to [the github repository](https://github.com/OlehOnyshchak/WikipediaMultimodalDownloader#docker)\n",
    "\n",
    "## Constant definition\n",
    "\n",
    "Fist of all, we need to specify some input parameters to our script. That is, input file with articles we want to download + parameters on how to download them.\n",
    "\n",
    "### Input file\n",
    "It should be a file with article ids specified one per line. By article id here we mean last part of its URL. That is, for the article with URL https://en.wikipedia.org/wiki/The_Relapse on English Wikipedia, the id would be `The_Relapse`. Please note, that all article ids you specified in a file should be from the same Wikipedia, i.e. either all English or all Ukrainian.\n",
    "\n",
    "Below you can see an example of how we setup a sample input file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = ['Butch_Levy']\n",
    "\n",
    "!rm input.txt > /dev/null 2>&1\n",
    "article_to_examine = articles[0]\n",
    "for a in articles:\n",
    "    ! echo $a >> input.txt\n",
    "! cat input.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters\n",
    "Then you need to specify a variety of parameters to fine-tune the collection script. On a high level, the script accounts to any previously downloaded information to increase the performance. In other words, once you downloaded a dataset from scratch, updating it will take very little time. That is because most of the data will be already downloaded and unmodified since the last collection.\n",
    "\n",
    "In other words, since script leverages cache, you can interrupt and then restart the collection script at any time without the need to start everything from scratch. You can also specify, what precisely you want the script to do 1) download missing articles and images from the input file 2) check that image metadata of already downloaded articles is up to date 3) force redownload off all images and/or image metadata and/or article text content. You can also execute the script from multiple notebooks/consoles with the same output directory to parallelise the collection process. That will significantly reduce the download time, although you need to beware that not any (offset, limit) parameters overlap. We are planning to add support of multithreading in the future, so now it's the only workaround. If you more details on parameters, please refer to documentation in the corresponding python file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import reader\n",
    "import data_preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Please refere to reader.py and data_preprocessor.py for documentation\n",
    "\n",
    "filename = 'input.txt' # 'featured_articles_list.tsv' # 'input.txt'\n",
    "out_dir = '/home/oleh/data_docker/'  #'../WikiImageRecommendation/data/' \n",
    "\n",
    "invalidate_headings_cache = True\n",
    "invalidate_parsed_titles_cache = False\n",
    "invalidate_visual_features_cache = False\n",
    "\n",
    "query_params = reader.QueryParams(\n",
    "    out_dir = out_dir,\n",
    "    debug_info = True,\n",
    "    offset = 0,\n",
    "    limit = None,\n",
    "    invalidate_cache = reader.InvalidateCacheParams(\n",
    "        img_cache = False,\n",
    "        text_cache = False,\n",
    "        caption_cache = False,\n",
    "        img_meta_cache = False,\n",
    "        oudated_img_meta_cache = True,  \n",
    "    ),\n",
    "    only_update_cached_pages = False,\n",
    "    fill_property= reader.FillPropertyParams(\n",
    "        img_caption = True,\n",
    "        img_description = True,\n",
    "        text_wikitext = False,\n",
    "        text_html = True,\n",
    "    ),\n",
    "    language_code = 'en',\n",
    "    early_icons_removal = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection\n",
    "In this section, the script will download all the required data. That is, for each specified article it will download its textual content, all its images and also some image metadata such as description parsed from Wikimedia Commons page. For details about what is being collected and what is the structure of the dataset, please refer to [Kaggle Dataset Page](https://www.kaggle.com/jacksoncrow/extended-wikipedia-multimodal-dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "reader.query(filename=filename, params=query_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing 1. Removing images not available on Commons\n",
    "Before proceeding to the costly operation of additional image caption downloading&parsing, we will first remove all images not available from Wikimedia Commons dataset. Usually, those are the images which were licensed only for usage in a specific article and are not publicly available. They constitute around 5-7% of pictures, so for now, we are just removing them. Still, later we might investigate licensing condition and, if allowed, include them to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_preprocessor.filter_img_metadata(\n",
    "    data_path=query_params.out_dir,\n",
    "    offset=query_params.offset,\n",
    "    limit=query_params.limit, \n",
    "    debug_info=query_params.debug_info,\n",
    "    field_to_remove='on_commons',\n",
    "    predicate=lambda x: ('on_commons' not in x) or x['on_commons'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing 2. Removing icons\n",
    "Most commonly, an icon is an auxiliary image which represents a particular template or category. It's not directly linked to the content described in the article, so we remove icons as noisy data. We identify them within other images under the assumption that user cannot load preview for icons on Wiki page. That is, if you click on icon from your browser, it will either do nothing or will redirect you to another page. While for images used in the article, it will load a full-screen preview. And while this approach will not work in 100% of cases, currently we identified it as the most reliable approach to perform icon identification.\n",
    "\n",
    "So in this part, we remove all images which were identified as icons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_preprocessor.filter_img_metadata(\n",
    "    data_path=query_params.out_dir,\n",
    "    offset=query_params.offset,\n",
    "    limit=query_params.limit, \n",
    "    debug_info=query_params.debug_info,\n",
    "    field_to_remove='is_icon',\n",
    "    predicate=lambda x: ('is_icon' not in x) or (not x['is_icon']),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing 3. Parsing Image Headings\n",
    "For each image in the article, it parses all its parent headings from the article's html. In other words, if a picture is located in a block with title `<h3>Title_3</h3>`, then @headings field of the metadata will contain heading of the first, second and third level respectively, i.e. `[\"Title_1\", \"Title_2\", \"Title_3\"]`. We parse the entire tree because only with all that context headings have sense and show extra information. Thus you might consider joining all of them into a single space-separated descriptive sentence.\n",
    "\n",
    "Again, if you need further details, please refer to [Kaggle Dataset Page](https://www.kaggle.com/jacksoncrow/extended-wikipedia-multimodal-dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "data_preprocessor.parse_image_headings(\n",
    "    data_path=query_params.out_dir,\n",
    "    offset=query_params.offset,\n",
    "    limit=query_params.limit,\n",
    "    invalidate_cache=invalidate_headings_cache,\n",
    "    debug_info=query_params.debug_info,\n",
    "    language_code=query_params.language_code,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing 4. Generating visual features\n",
    "Lastly, to make usage of dataset more time and space-efficient, we will calculate visual features for every image and record them in the dataset. By doing so, we will\n",
    "* save space: the raw image of shape (600,600,3) occupies 500 times more space than a visual feature vector with 2048 elements. At the same time, it provides the same amount of useful information\n",
    "* save time: calculating those features from scratch is a very time-consuming process. So by having them saved in the dataset, every user of the dataset will not need to calculate them as well.\n",
    "\n",
    "For feature generation we used `ResNet152` pretrained on `ImageNet` dataset. And features themselves are the output of the lash hidden fully-connected layer of the network, which has the shape of (19, 24, 2048), and then transform it to a vector of 2048 items by the max-pooling operation. That vector of 2048 items will serve as our feature vector for each image.\n",
    "\n",
    "And while we understand that this representation might not be ideal in your scenario, it seems to be useful in various situation. If you need to calculate features in another way, please just modify this last step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "data_preprocessor.generate_visual_features(\n",
    "    data_path=query_params.out_dir,\n",
    "    offset=query_params.offset,\n",
    "    limit=query_params.limit,\n",
    "    invalidate_cache=invalidate_visual_features_cache,\n",
    "    debug_info=query_params.debug_info,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing 5. Parse image titles\n",
    "Image titles often contain a short meaningful description of an image. Although, it is also commonly written in a form where words are either _separated_by_underscore_symbol_, or written in _camelCase_, or simply _writtenwithoutspaces_. So to extract useful features from that title, we will try to guess each separate word of a title and record it into `processed_title` field. We will do it with redditscore.tokenizer, which parses the string based on known words and their frequency in language. In other words, if a sentence can be parsed into a few possible alternatives, the one with more frequently used terms will take priority"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "data_preprocessor.tokenize_image_titles(\n",
    "    data_path=query_params.out_dir,\n",
    "    offset=query_params.offset,\n",
    "    limit=query_params.limit,\n",
    "    invalidate_cache=invalidate_parsed_titles_cache,\n",
    "    debug_info=query_params.debug_info,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Examinations\n",
    "### text.json file\n",
    "This file contains article textual information, such as: content of the article in wikitext and html format, article title, id, and url. For further details, please refer to [Kaggle Dataset Page](https://www.kaggle.com/jacksoncrow/extended-wikipedia-multimodal-dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pprint\n",
    "\n",
    "text_path = out_dir + article_to_examine + \"/text.json\"\n",
    "pp = pprint.PrettyPrinter(indent=2)\n",
    "data = None\n",
    "with open(text_path) as json_file:\n",
    "    data = json.loads(json.load(json_file))\n",
    "\n",
    "print_data = data\n",
    "if 'wikitext' in print_data:\n",
    "    print_data['wikitext'] = print_data['wikitext'][:5000]\n",
    "\n",
    "if 'html' in print_data:\n",
    "    print_data['html'] = print_data['html'][:5000]\n",
    "\n",
    "pp.pprint(print_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### meta.json file\n",
    "This file contains visual features of all articles images as well as some image metadata such as: description from Commons dataset, caption from the article, title, url and filename. For further details, please refer to [Kaggle Dataset Page](https://www.kaggle.com/jacksoncrow/extended-wikipedia-multimodal-dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pprint\n",
    "\n",
    "meta_path = out_dir + article_to_examine + \"/img/meta.json\"\n",
    "pp = pprint.PrettyPrinter(indent=2)\n",
    "data = None\n",
    "with open(meta_path) as json_file:\n",
    "    data = json.loads(json.load(json_file))['img_meta']\n",
    "\n",
    "print_data = data\n",
    "for i in range(len(print_data)):\n",
    "    if 'features' in print_data[i]:\n",
    "        print_data[i]['features'] = print_data[i]['features'][:10]\n",
    "print_data = {i:x for i,x in enumerate(print_data)}\n",
    "\n",
    "pp.pprint(print_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
